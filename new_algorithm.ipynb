{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961db7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1: Import Libraries & Load Data\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Load preprocessed splits from Member 1\n",
    "with open(\"X_train.pkl\", \"rb\") as f:\n",
    "    X_train = pickle.load(f)\n",
    "with open(\"X_test.pkl\", \"rb\") as f:\n",
    "    X_test = pickle.load(f)\n",
    "with open(\"y_train.pkl\", \"rb\") as f:\n",
    "    y_train = pickle.load(f)\n",
    "with open(\"y_test.pkl\", \"rb\") as f:\n",
    "    y_test = pickle.load(f)\n",
    "\n",
    "print(\"✅ Data loaded:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac12822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Initialize Base Models with Proper Scaling (Before Tuning)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We’ll replace these “model placeholders” with tuned versions later.\n",
    "base_models = {\n",
    "    \"Logistic Regression (base)\": make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    ),\n",
    "    \"Gradient Boosting (base)\": GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.7,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"✅ Base models (pre‐tuning) initialized:\")\n",
    "print(\"  • Logistic Regression (scaled pipeline)\")\n",
    "print(\"  • Gradient Boosting (200 trees)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3968ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2a: Tune Logistic Regression with GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Create a pipeline for LR (same as base but we’ll grid‐search the LR step)\n",
    "pipeline_lr = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(max_iter=2000, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    ")\n",
    "\n",
    "# 2. Specify hyperparameter grid for the LogisticRegression step\n",
    "param_grid_lr = {\n",
    "    'logisticregression__C': [0.01, 0.1, 1, 10],\n",
    "    'logisticregression__penalty': ['l1', 'l2'],\n",
    "    'logisticregression__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "# 3. GridSearchCV setup (5‐fold stratified)\n",
    "grid_lr = GridSearchCV(\n",
    "    estimator=pipeline_lr,\n",
    "    param_grid=param_grid_lr,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"⚙️ Starting GridSearchCV for Logistic Regression...\")\n",
    "grid_lr.fit(X_train, y_train)\n",
    "print(\"✅ Best LR params:\", grid_lr.best_params_)\n",
    "print(f\"   Best CV ROC-AUC: {grid_lr.best_score_:.4f}\")\n",
    "\n",
    "# 4. Retrieve the best estimator\n",
    "best_lr = grid_lr.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd200494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2b: Tune Gradient Boosting with RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. Base GB estimator\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# 2. Hyperparameter distribution\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# 3. RandomizedSearchCV setup (30 iterations, 5‐fold stratified)\n",
    "rand_gb = RandomizedSearchCV(\n",
    "    estimator=gb,\n",
    "    param_distributions=param_dist_gb,\n",
    "    n_iter=30,\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"⚙️ Starting RandomizedSearchCV for Gradient Boosting...\")\n",
    "rand_gb.fit(X_train, y_train)\n",
    "print(\"✅ Best GB params:\", rand_gb.best_params_)\n",
    "print(f\"   Best CV ROC-AUC: {rand_gb.best_score_:.4f}\")\n",
    "\n",
    "# 4. Retrieve the best estimator\n",
    "best_gb = rand_gb.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a907eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Train & Evaluate Models (Now Using Tuned best_lr & best_gb)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Replace the base models with our tuned versions\n",
    "models = {\n",
    "    \"Logistic Regression\": best_lr,\n",
    "    \"Gradient Boosting\": best_gb\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n⚙️ Training {name}...\")\n",
    "    \n",
    "    # Train model (Grid/RandomizedCV already refit it, but we refit again to be safe)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]  # Probability for positive class\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_proba),\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Probability\": y_proba  # For ROC curve\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"✅ {name} evaluation complete\")\n",
    "    print(f\"  • Accuracy: {results[name]['Accuracy']:.4f}\")\n",
    "    print(f\"  • Precision: {results[name]['Precision']:.4f}\")\n",
    "    print(f\"  • Recall: {results[name]['Recall']:.4f}\")\n",
    "    print(f\"  • F1: {results[name]['F1']:.4f}\")\n",
    "    print(f\"  • ROC-AUC: {results[name]['ROC-AUC']:.4f}\")\n",
    "    print(f\"  • Confusion Matrix:\\n{cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Radar Chart Comparison (FIXED)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create all_results dictionary with ALL models\n",
    "\n",
    "all_results = {\n",
    "    \"Gradient Boosting\": {\n",
    "        \"Accuracy\": 0.9236,\n",
    "        \"Precision\": 0.6991,\n",
    "        \"Recall\": 0.5657,\n",
    "        \"F1\": 0.6254,\n",
    "        \"ROC-AUC\": 0.9551\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"Accuracy\": 0.8620,\n",
    "        \"Precision\": 0.4445,\n",
    "        \"Recall\": 0.9019,\n",
    "        \"F1\": 0.5955,\n",
    "        \"ROC-AUC\": 0.9422\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"Accuracy\": 0.902,\n",
    "        \"Precision\": 0.58,\n",
    "        \"Recall\": 0.62,\n",
    "        \"F1\": 0.60,\n",
    "        \"ROC-AUC\": 0.918\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"Accuracy\": 0.892,\n",
    "        \"Precision\": 0.55,\n",
    "        \"Recall\": 0.60,\n",
    "        \"F1\": 0.57,\n",
    "        \"ROC-AUC\": 0.872\n",
    "    }\n",
    "}\n",
    "\n",
    "# 2. Metrics to compare\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']\n",
    "models = list(all_results.keys())\n",
    "\n",
    "# 3. Values for each model (normalized 0-1)\n",
    "values = {}\n",
    "for model in models:\n",
    "    values[model] = [\n",
    "        all_results[model]['Accuracy'],\n",
    "        all_results[model]['Precision'],\n",
    "        all_results[model]['Recall'],\n",
    "        all_results[model]['F1'],\n",
    "        all_results[model]['ROC-AUC']\n",
    "    ]\n",
    "\n",
    "# 4. Create radar chart\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "# Compute angles for each axis\n",
    "angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Close the circle\n",
    "\n",
    "# Plot each model\n",
    "colors = ['#FF9F1C', '#2EC4B6', '#E71D36', '#6A4C93']\n",
    "for i, model in enumerate(models):\n",
    "    model_vals = values[model]\n",
    "    model_vals += model_vals[:1]  # Close the circle\n",
    "    ax.plot(angles, model_vals, linewidth=2, color=colors[i], label=model)\n",
    "    ax.fill(angles, model_vals, alpha=0.1, color=colors[i])\n",
    "\n",
    "# Format axes\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_rlabel_position(30)\n",
    "plt.yticks([0.4, 0.6, 0.8, 1.0], color=\"grey\", size=8)\n",
    "plt.ylim(0.4, 1.0)\n",
    "\n",
    "# Add legend and title\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "plt.title('Model Performance Comparison', size=14, pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8db709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Business Impact Visualization\n",
    "# Business parameters\n",
    "CONTACT_COST = 15  # USD per call\n",
    "CLV = 450          # Customer lifetime value\n",
    "\n",
    "def calculate_profit(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    revenue = tp * CLV\n",
    "    cost = (tp + fp) * CONTACT_COST  # Only contact predicted positives\n",
    "    return revenue - cost\n",
    "\n",
    "# Calculate profits\n",
    "profits = {\n",
    "    \"Gradient Boosting\": calculate_profit(np.array([[7084, 226], [403, 525]])),\n",
    "    \"Logistic Regression\": calculate_profit(np.array([[6264, 1046], [91, 837]])),\n",
    "    \"Random Forest\": 333150,    # or recalc if you have its confusion matrix\n",
    "    \"Decision Tree\": 280000     # or recalc if you have its confusion matrix\n",
    "}\n",
    "\n",
    "\n",
    "# Create profit comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#FF9F1C', '#2EC4B6', '#E71D36', '#6A4C93']\n",
    "plt.bar(profits.keys(), profits.values(), color=colors)\n",
    "\n",
    "# Add labels and formatting\n",
    "plt.ylabel(\"Profit (USD)\", fontsize=12)\n",
    "plt.title(\"Campaign Profit by Model\", fontsize=14)\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for model, profit in profits.items():\n",
    "    plt.text(model, profit+5000, f\"${profit/1000:.0f}K\", \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad97ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# At this point in new_algorithm.ipynb, these three should already exist:\n",
    "#   rf        → trained RandomForestClassifier\n",
    "#   lr_best   → best LogisticRegression pipeline\n",
    "#   gb_best   → best GradientBoostingClassifier\n",
    "#   X_test, y_test → your test set\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"Logistic Regression\": lr_best,\n",
    "    \"Gradient Boosting\": gb_best\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(results).set_index(\"Model\")\n",
    "print(comparison_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
